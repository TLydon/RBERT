% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf2-layer_attention.R
\name{custom_layer_attention}
\alias{custom_layer_attention}
\title{Custom Layer: Attention}
\usage{
custom_layer_attention(
  object,
  name = NULL,
  trainable = NULL,
  param_list = list(),
  ...
)
}
\arguments{
\item{object}{Model or layer object.}

\item{name}{Character; An optional name for the layer. Must be unique in a
model.}

\item{trainable}{Logical; whether the layer weights will be updated during
training.}

\item{param_list}{A named list of parameter values used in defining the
layer.
\describe{
\item{\code{dtype}}{The data type of the layer output.
Defaults to "float32". Valid values from \code{tensorflow::tf$float32$name},
etc. }
}}
}
\description{
Create first part of self attention layer. Takes as input an embeddings layer
(could be the output of a previous attention layer), performs self attention,
adds input layer back via residual connection, and applies layer
normalization.
}
\details{
In an encoder, this layer is typically followed by an intermediate dense
layer, with a redsidual connection before another layer normalization.

Note that this layer implements more of the attention mechanism than
does \code{keras::layer_attention}.
}
