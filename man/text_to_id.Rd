% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{text_to_id}
\alias{text_to_id}
\title{Tokenize Text with Word Pieces}
\usage{
text_to_id(
  text,
  ckpt_dir = NULL,
  vocab_file = find_vocab(ckpt_dir),
  include_special = TRUE
)
}
\arguments{
\item{text}{Character vector; text to tokenize.}

\item{include_special}{Logical; whether to add the special tokens "[CLS]" (at
the beginning) and "[SEP]" (at the end) of the token list.}
}
\value{
A list of character vectors, giving the tokenization of the input
  text.
}
\description{
Given some text and a word piece vocabulary, IDifies the text. This is
primarily a tool for quickly checking the IDification of a piece of text.
}
\examples{
\dontrun{
BERT_PRETRAINED_DIR <- download_BERT_checkpoint("bert_base_uncased")
tokens <- text_to_id(
  text = c("Who doesn't like tacos?", "Not me!"),
  ckpt_dir = BERT_PRETRAINED_DIR
)
}
}
