% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf2-layer_proj_add_norm.R
\name{custom_layer_proj_add_norm}
\alias{custom_layer_proj_add_norm}
\title{Custom Layer: Project, Add, Normalize}
\usage{
custom_layer_proj_add_norm(
  object,
  name = NULL,
  trainable = NULL,
  param_list = list(),
  ...
)
}
\arguments{
\item{object}{Model or layer object.}

\item{name}{Character; An optional name for the layer. Must be unique in a
model.}

\item{trainable}{Logical; whether the layer weights will be updated during
training.}

\item{param_list}{A named list of parameter values used in defining the
layer.
\describe{
\item{\code{hidden_size}}{Integer; The size of the output. Should match the
size of the second input.}
\item{\code{hidden_dropout}}{Numeric; the dropout rate (fraction to drop)
applied after the dense layer projection.}
\item{\code{initializer_range}}{Numeric; the value passed in as the \code{stddev}
parameter to the \code{initializer_truncated_normal} in the dense layer
initializer.}
\item{\code{dtype}}{The data type of the layer output. Defaults to "float32".
Valid values from \code{tensorflow::tf$float32$name}, etc. }
}}
}
\description{
Create a layer that, given two input layers, applies a dense layer projection
(followed by dropout) to the first input, then adds the second (as a
residual) and normalizes the sum.
}
