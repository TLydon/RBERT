% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{gelu}
\alias{gelu}
\title{Gaussian Error Linear Unit}
\usage{
gelu(x)
}
\arguments{
\item{x}{Float Tensor to perform activation on.}
}
\value{
`x` with the GELU activation applied.
}
\description{
This is a smoother version of the RELU. Original paper:
https://arxiv.org/abs/1606.08415
}
\examples{
\dontrun{
tfx <- tensorflow::tf$get_variable("none", tensorflow::shape(10L))
gelu(tfx)
}
}
