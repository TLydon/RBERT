% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeling.R
\name{layer_norm}
\alias{layer_norm}
\title{Run layer normalization}
\usage{
layer_norm(input_tensor, name = NULL)
}
\arguments{
\item{input_tensor}{Tensor to perform layor normalization on.}

\item{name}{Optional variable_scope for layer_norm.}
}
\value{
A Tensor of the same shape and type as `input_tensor`, with
normalization applied.
}
\description{
Run layer normalization on the last dimension of the tensor.
}
\details{
Wrapper around tensorflow layer_norm function. From tensorflow documentation:
Adds a Layer Normalization layer. Based on the paper:
\url{https://arxiv.org/abs/1607.06450}.

Note: \code{begin_norm_axis}: The first normalization dimension:
normalization will be performed along dimensions (begin_norm_axis :
rank(inputs) )

\code{begin_params_axis}: The first parameter (beta, gamma) dimension: scale
and centering parameters will have dimensions (begin_params_axis :
rank(inputs) ) and will be broadcast with the normalized inputs accordingly.
}
\examples{
\dontrun{
tfx <- tensorflow::tf$compat$v1$get_variable("example", tensorflow::shape(10L))
layer_norm(tfx)
}
}
